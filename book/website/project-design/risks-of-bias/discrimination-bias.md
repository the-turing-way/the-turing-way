(pd-risks-of-bias-discrimination-bias)=
# Discrimination and Bias

The word *bias* is often used to refer to prejudice against a group of people. 
While this differs from the statistical bias which has been the focus of this chapter, there is significant overlap between these types of bias. 
A model may act in a biased way against a group of people, which can lead to discriminatory outcomes. 

Data science and AI will always be at risk of bias - biased data leads to biased models, which may lead to biased decision-making. 
Datasets contain biases which reflect the personal and societal biases which were present during data collection.
They also become biased due to difficulties in {ref}`sampling <pd-risks-of-bias-identifying-risks-of-bias-sampling>`. 

Examples of discrimination in AI include [racism in predictive policing](https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/), [sexism in recruiting](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G/), and multiple forms of [discrimination in generative AI](https://en.wikipedia.org/wiki/Tay_(chatbot)). 
These algorithms form negative feedback loops, with historic discrimination contributing to discrimination in future decision-making. 
There is [no one-size fits all solution](https://dl.acm.org/doi/10.1145/3616865) to ensure fairness in AI, though guidance is available through the [Information Commissioner's Office (ICO)](https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/guidance-on-ai-and-data-protection/how-do-we-ensure-fairness-in-ai/) and the [Alan Turing Institute](https://www.turing.ac.uk/news/publications/ai-ethics-and-governance-practice-ai-fairness-practice). 

Further information can also be found through the [Turing-Roche Knowledge Share Event: Fairness in AI for Health](https://www.youtube.com/watch?v=Tuz7IGqDAIs&list=PLDbZND-EA4eHYGwDyOEiUumyCNPAx2gyF).
 
